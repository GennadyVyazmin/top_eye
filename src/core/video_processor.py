# /top_eye/src/core/video_processor_reid.py
import cv2
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import numpy as np
from threading import Thread, Lock
from queue import Queue
from datetime import datetime
import time
import os
import pickle
from collections import deque, defaultdict
from sklearn.neighbors import NearestNeighbors
from scipy.spatial.distance import cosine
import warnings

warnings.filterwarnings('ignore')


class ReIDModel(nn.Module):
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è ReID –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ ResNet"""

    def __init__(self):
        super(ReIDModel, self).__init__()
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π ResNet
        from torchvision import models
        self.backbone = models.resnet18(pretrained=True)
        # –£–±–∏—Ä–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π
        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])
        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —á–∞—Å—Ç—å —Å–ª–æ–µ–≤
        for param in list(self.backbone.parameters())[:-10]:
            param.requires_grad = False

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–∏ –¥–ª—è ReID
        self.reid_head = nn.Sequential(
            nn.Linear(512, 256),
            nn.BatchNorm1d(256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, 128)  # 128-–º–µ—Ä–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥
        )

    def forward(self, x):
        features = self.backbone(x)
        features = features.view(features.size(0), -1)
        embeddings = self.reid_head(features)
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings = nn.functional.normalize(embeddings, p=2, dim=1)
        return embeddings


class VideoProcessor:
    def __init__(self, config):
        self.config = config
        print(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ –¥–ª—è –∫–∞–º–µ—Ä—ã: {config.CAMERA_ID}")

        # –ö–∞–º–µ—Ä–∞
        self.cap = None
        self.last_reconnect = time.time()
        self.reconnect_interval = 5

        # –û—á–µ—Ä–µ–¥–∏
        self.frame_queue = Queue(maxsize=20)
        self.processed_queue = Queue(maxsize=20)
        self.lock = Lock()
        self.running = False

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.current_count = 0
        self.today_unique = set()
        self.session_unique = set()
        self.visitor_embeddings = {}  # {track_id: embeddings_history}
        self.visitor_appearances = {}  # {track_id: appearance_samples}

        # –ú–æ–¥–µ–ª–∏
        self.model = None  # YOLO
        self.reid_model = None  # ReID –º–æ–¥–µ–ª—å
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º—ã –¥–ª—è ReID
        self.transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.Resize((256, 128)),  # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–ª—è ReID
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                 std=[0.229, 0.224, 0.225])
        ])

        # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç—Ä–µ–∫–∏–Ω–≥
        self.active_tracks = {}
        self.lost_tracks = {}  # –ü–æ—Ç–µ—Ä—è–Ω–Ω—ã–µ —Ç—Ä–µ–∫–∏ (–∫—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω–æ)
        self.next_track_id = 1000
        self.track_counter = 0

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ç—Ä–µ–∫–∏–Ω–≥–∞
        self.max_age = 30  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–æ–∑—Ä–∞—Å—Ç —Ç—Ä–µ–∫–∞ –±–µ–∑ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        self.min_hits = 3  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø–∞–¥–∞–Ω–∏–π –¥–ª—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
        self.iou_threshold = 0.3
        self.reid_threshold = 0.7  # –ü–æ—Ä–æ–≥ –¥–ª—è ReID —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        self.max_features_per_track = 10  # –ú–∞–∫—Å–∏–º—É–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ —Ç—Ä–µ–∫

        # –ö—ç—à –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.embedding_cache = {}

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        self.init_models()
        print(f"‚úì –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.device}")

    def init_models(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π"""
        try:
            print("–ó–∞–≥—Ä—É–∑–∫–∞ YOLO...")
            from ultralytics import YOLO
            model_path = self.config.YOLO_MODEL_PATH

            if not os.path.exists(model_path):
                print("–°–∫–∞—á–∏–≤–∞–µ–º YOLOv8n...")
                model_path = 'yolov8n.pt'

            self.model = YOLO(model_path)
            self.model.to(self.device)
            print(f"‚úì YOLO –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–∞ {self.device}")

            print("–ó–∞–≥—Ä—É–∑–∫–∞ ReID –º–æ–¥–µ–ª–∏...")
            self.reid_model = ReIDModel().to(self.device)
            self.reid_model.eval()  # –†–µ–∂–∏–º inference
            print("‚úì ReID –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
            reid_weights = os.path.join(os.path.dirname(__file__), "../../models/reid_weights.pth")
            if os.path.exists(reid_weights):
                self.reid_model.load_state_dict(torch.load(reid_weights, map_location=self.device))
                print("‚úì –í–µ—Å–∞ ReID –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

        except Exception as e:
            print(f"‚úó –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            import traceback
            traceback.print_exc()

    def start(self):
        """–ó–∞–ø—É—Å–∫"""
        self.running = True
        Thread(target=self._capture_frames, daemon=True).start()
        Thread(target=self._process_frames, daemon=True).start()
        Thread(target=self._manage_tracks, daemon=True).start()
        print("‚úì –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—É—â–µ–Ω–∞")

    def _capture_frames(self):
        """–ó–∞—Ö–≤–∞—Ç –∫–∞–¥—Ä–æ–≤"""
        while self.running:
            try:
                if self.cap is None or not self.cap.isOpened():
                    if time.time() - self.last_reconnect > self.reconnect_interval:
                        self._reconnect_camera()
                        time.sleep(1)
                    continue

                success, frame = self.cap.read()
                if not success:
                    self.cap.release()
                    self.cap = None
                    continue

                if not self.frame_queue.full():
                    self.frame_queue.put(frame.copy())

                time.sleep(0.01)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞—Ö–≤–∞—Ç–∞: {e}")
                if self.cap:
                    self.cap.release()
                self.cap = None
                time.sleep(1)

    def _reconnect_camera(self):
        """–ü–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ"""
        try:
            if self.cap:
                self.cap.release()

            print(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫: {self.config.RTSP_URL}")
            self.cap = cv2.VideoCapture(self.config.RTSP_URL)
            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)
            self.cap.set(cv2.CAP_PROP_FPS, self.config.FPS)

            if self.cap.isOpened():
                print("‚úì –ö–∞–º–µ—Ä–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∞")
                self.last_reconnect = time.time()
                return True
            else:
                print("‚úó –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è")
                return False

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
            return False

    def _process_frames(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–æ–≤"""
        while self.running:
            try:
                if not self.frame_queue.empty():
                    frame = self.frame_queue.get()
                    processed = self._process_single_frame(frame)

                    if not self.processed_queue.full():
                        self.processed_queue.put(processed)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")

    def _manage_tracks(self):
        """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–µ–∫–∞–º–∏"""
        while self.running:
            try:
                current_time = time.time()

                # –û–±–Ω–æ–≤–ª—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç —Ç—Ä–µ–∫–æ–≤
                tracks_to_remove = []
                for track_id, track in list(self.active_tracks.items()):
                    if current_time - track['last_seen'] > self.max_age / 10:
                        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤ –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã–µ
                        self.lost_tracks[track_id] = {
                            **track,
                            'lost_since': current_time
                        }
                        tracks_to_remove.append(track_id)
                        print(f"–¢—Ä–µ–∫ {track_id} –ø–µ—Ä–µ–º–µ—â–µ–Ω –≤ –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã–µ")

                for track_id in tracks_to_remove:
                    del self.active_tracks[track_id]

                # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã–µ —Ç—Ä–µ–∫–∏
                lost_to_remove = []
                for track_id, track in list(self.lost_tracks.items()):
                    if current_time - track['lost_since'] > 5:  # 5 —Å–µ–∫—É–Ω–¥
                        lost_to_remove.append(track_id)

                for track_id in lost_to_remove:
                    del self.lost_tracks[track_id]

                time.sleep(0.5)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–∫–∞–º–∏: {e}")
                time.sleep(1)

    def _process_single_frame(self, frame):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞"""
        result = {
            'frame': frame.copy(),
            'detections': [],
            'timestamp': datetime.now(),
            'people_count': 0,
            'fps': 0,
            'track_info': []
        }

        try:
            start_time = time.time()

            # YOLO –¥–µ—Ç–µ–∫—Ü–∏—è
            with torch.no_grad():
                yolo_results = self.model(
                    frame,
                    conf=self.config.CONFIDENCE_THRESHOLD,
                    device=self.device,
                    verbose=False,
                    classes=[0]
                )

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏
            current_detections = []
            if yolo_results and len(yolo_results) > 0:
                yolo_result = yolo_results[0]

                if yolo_result.boxes is not None and len(yolo_result.boxes) > 0:
                    boxes = yolo_result.boxes.xyxy.cpu().numpy()
                    confidences = yolo_result.boxes.conf.cpu().numpy()

                    for i in range(len(boxes)):
                        x1, y1, x2, y2 = boxes[i].astype(int)
                        conf = float(confidences[i])

                        width = x2 - x1
                        height = y2 - y1

                        if width > 40 and height > 80:  # –§–∏–ª—å—Ç—Ä —Ä–∞–∑–º–µ—Ä–∞
                            # –í—ã—Ä–µ–∑–∞–µ–º –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
                            person_roi = frame[y1:y2, x1:x2]

                            # –ü–æ–ª—É—á–∞–µ–º ReID —ç–º–±–µ–¥–¥–∏–Ω–≥
                            embedding = self._get_embedding(person_roi)

                            if embedding is not None:
                                current_detections.append({
                                    'bbox': [float(x1), float(y1), float(x2), float(y2)],
                                    'center': [(x1 + x2) / 2, (y1 + y2) / 2],
                                    'confidence': conf,
                                    'embedding': embedding,
                                    'roi': person_roi,
                                    'width': width,
                                    'height': height
                                })

            # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç—Ä–µ–∫–∏–Ω–≥
            tracked = self._advanced_tracking(current_detections)

            # –û–±–Ω–æ–≤–ª—è–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ —Ç—Ä–µ–∫–∏
            current_time = time.time()
            for det in tracked:
                track_id = det['track_id']

                if track_id not in self.active_tracks:
                    # –ù–æ–≤—ã–π —Ç—Ä–µ–∫
                    self.active_tracks[track_id] = {
                        'bbox': det['bbox'],
                        'last_seen': current_time,
                        'first_seen': current_time,
                        'embeddings': [det['embedding']],
                        'hits': 1,
                        'age': 1,
                        'color': self._get_random_color(track_id)
                    }

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–Ω–µ—à–Ω–∏–π –≤–∏–¥
                    self.visitor_appearances[track_id] = [det['roi']]
                else:
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç—Ä–µ–∫
                    track = self.active_tracks[track_id]

                    # –û–±–Ω–æ–≤–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (—Å–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ)
                    if len(track['embeddings']) >= self.max_features_per_track:
                        track['embeddings'].pop(0)
                    track['embeddings'].append(det['embedding'])

                    # –û–±–Ω–æ–≤–ª—è–µ–º –≤–Ω–µ—à–Ω–∏–π –≤–∏–¥
                    if track_id in self.visitor_appearances:
                        if len(self.visitor_appearances[track_id]) < 5:
                            self.visitor_appearances[track_id].append(det['roi'])

                    track.update({
                        'bbox': det['bbox'],
                        'last_seen': current_time,
                        'hits': track['hits'] + 1,
                        'age': track['age'] + 1
                    })

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                result['detections'].append({
                    'track_id': track_id,
                    'bbox': det['bbox'],
                    'confidence': det['confidence'],
                    'age': self.active_tracks[track_id]['age'],
                    'hits': self.active_tracks[track_id]['hits']
                })

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            result['people_count'] = len(result['detections'])
            self.current_count = result['people_count']

            # –û–±–Ω–æ–≤–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö
            for det in result['detections']:
                track_id = det['track_id']
                if self.active_tracks[track_id]['hits'] > 10:
                    self.session_unique.add(track_id)
                    today = datetime.now().date().isoformat()
                    self.today_unique.add(f"{today}_{track_id}")

            # FPS
            end_time = time.time()
            result['fps'] = 1.0 / (end_time - start_time) if (end_time - start_time) > 0 else 0

            # –†–∏—Å—É–µ–º
            self._draw_detections(frame, result['detections'])

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–∞–¥—Ä–∞: {e}")
            import traceback
            traceback.print_exc()

        return result

    def _get_embedding(self, image):
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        if image is None or image.size == 0:
            return None

        try:
            # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ —Ö–µ—à—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            img_hash = hashlib.md5(image.tobytes()).hexdigest()
            if img_hash in self.embedding_cache:
                return self.embedding_cache[img_hash]

            # –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥
            transformed = self.transform(image).unsqueeze(0).to(self.device)

            # Inference
            with torch.no_grad():
                embedding = self.reid_model(transformed)
                embedding = embedding.cpu().numpy().flatten()

            # –ö—ç—à–∏—Ä—É–µ–º
            self.embedding_cache[img_hash] = embedding
            if len(self.embedding_cache) > 1000:
                # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
                keys = list(self.embedding_cache.keys())
                for key in keys[:-500]:
                    del self.embedding_cache[key]

            return embedding

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            return None

    def _advanced_tracking(self, current_detections):
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç—Ä–µ–∫–∏–Ω–≥ —Å ReID"""
        tracked_detections = []

        if not current_detections:
            return tracked_detections

        # 1. –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å –∞–∫—Ç–∏–≤–Ω—ã–º–∏ —Ç—Ä–µ–∫–∞–º–∏
        matched_detections = set()
        matched_tracks = set()

        if self.active_tracks:
            # –°–æ–∑–¥–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É —Å—Ö–æ–∂–µ—Å—Ç–∏
            similarity_matrix = []

            for i, det in enumerate(current_detections):
                for track_id, track in self.active_tracks.items():
                    # –í—ã—á–∏—Å–ª—è–µ–º IoU
                    iou = self._compute_iou(det['bbox'], track['bbox'])

                    # –í—ã—á–∏—Å–ª—è–µ–º ReID —Å—Ö–æ–∂–µ—Å—Ç—å
                    reid_similarity = 0
                    if det['embedding'] is not None and track['embeddings']:
                        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å–æ –≤—Å–µ–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —Ç—Ä–µ–∫–∞
                        similarities = []
                        for track_emb in track['embeddings']:
                            sim = 1 - cosine(det['embedding'], track_emb)
                            similarities.append(sim)
                        reid_similarity = max(similarities) if similarities else 0

                    # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π score
                    if iou > self.iou_threshold:
                        # –ü—Ä–∏ –≤—ã—Å–æ–∫–æ–º IoU –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                        score = 0.7 * min(1, iou / 0.5) + 0.3 * reid_similarity
                    else:
                        # –ü—Ä–∏ –Ω–∏–∑–∫–æ–º IoU –±–æ–ª—å—à–µ –ø–æ–ª–∞–≥–∞–µ–º—Å—è –Ω–∞ ReID
                        score = 0.3 * min(1, iou / 0.5) + 0.7 * reid_similarity

                    similarity_matrix.append((i, track_id, score, iou, reid_similarity))

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score
            similarity_matrix.sort(key=lambda x: x[2], reverse=True)

            # –ñ–∞–¥–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ
            for i, track_id, score, iou, reid_sim in similarity_matrix:
                if score > 0.4:  # –ü–æ—Ä–æ–≥ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
                    if i not in matched_detections and track_id not in matched_tracks:
                        det = current_detections[i]
                        det['track_id'] = track_id
                        det['match_score'] = score
                        det['iou'] = iou
                        det['reid_sim'] = reid_sim
                        tracked_detections.append(det)

                        matched_detections.add(i)
                        matched_tracks.add(track_id)

        # 2. –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—è–Ω–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤
        for i, det in enumerate(current_detections):
            if i in matched_detections:
                continue

            best_track_id = None
            best_score = 0

            for track_id, track in self.lost_tracks.items():
                if 'embeddings' in track and track['embeddings']:
                    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                    similarities = []
                    for track_emb in track['embeddings']:
                        if det['embedding'] is not None:
                            sim = 1 - cosine(det['embedding'], track_emb)
                            similarities.append(sim)

                    if similarities:
                        score = max(similarities)
                        if score > self.reid_threshold and score > best_score:
                            best_score = score
                            best_track_id = track_id

            if best_track_id:
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç—Ä–µ–∫
                det['track_id'] = best_track_id
                det['match_score'] = best_score
                det['recovered'] = True
                tracked_detections.append(det)

                matched_detections.add(i)

                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ –∞–∫—Ç–∏–≤–Ω—ã–µ
                self.active_tracks[best_track_id] = self.lost_tracks[best_track_id]
                self.active_tracks[best_track_id]['last_seen'] = time.time()
                self.active_tracks[best_track_id]['hits'] += 1
                self.active_tracks[best_track_id]['age'] += 1

                # –û–±–Ω–æ–≤–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                if det['embedding'] is not None:
                    if len(self.active_tracks[best_track_id]['embeddings']) >= self.max_features_per_track:
                        self.active_tracks[best_track_id]['embeddings'].pop(0)
                    self.active_tracks[best_track_id]['embeddings'].append(det['embedding'])

                del self.lost_tracks[best_track_id]
                print(f"‚úÖ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω —Ç—Ä–µ–∫ {best_track_id} (ReID score: {best_score:.3f})")

        # 3. –ù–æ–≤—ã–µ –¥–µ—Ç–µ–∫—Ü–∏–∏
        for i, det in enumerate(current_detections):
            if i not in matched_detections:
                track_id = self._get_new_track_id()
                det['track_id'] = track_id
                det['new'] = True
                tracked_detections.append(det)

        return tracked_detections

    def _compute_iou(self, box1, box2):
        """–í—ã—á–∏—Å–ª—è–µ—Ç IoU"""
        x1_min, y1_min, x1_max, y1_max = box1
        x2_min, y2_min, x2_max, y2_max = box2

        inter_x_min = max(x1_min, x2_min)
        inter_y_min = max(y1_min, y2_min)
        inter_x_max = min(x1_max, x2_max)
        inter_y_max = min(y1_max, y2_max)

        inter_area = max(0, inter_x_max - inter_x_min) * max(0, inter_y_max - inter_y_min)
        box1_area = (x1_max - x1_min) * (y1_max - y1_min)
        box2_area = (x2_max - x2_min) * (y2_max - y2_min)

        return inter_area / (box1_area + box2_area - inter_area + 1e-10)

    def _get_random_color(self, track_id):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ü–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ ID"""
        np.random.seed(track_id)
        return tuple(map(int, np.random.randint(0, 255, 3)))

    def _get_new_track_id(self):
        """–ù–æ–≤—ã–π ID"""
        track_id = self.next_track_id
        self.next_track_id += 1
        return track_id

    def _draw_detections(self, frame, detections):
        """–†–∏—Å—É–µ—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏"""
        for det in detections:
            bbox = det['bbox']
            track_id = det['track_id']
            confidence = det['confidence']
            age = det.get('age', 1)
            hits = det.get('hits', 1)

            x1, y1, x2, y2 = map(int, bbox)

            # –ü–æ–ª—É—á–∞–µ–º —Ü–≤–µ—Ç —Ç—Ä–µ–∫–∞
            color = self.active_tracks.get(track_id, {}).get('color', (0, 255, 0))

            # –ü—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫
            thickness = 2 if hits > 10 else 1
            cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)

            # ID –∏ confidence
            text = f"ID: {track_id}"
            if det.get('recovered'):
                text = f"‚Üª{track_id}"
            elif det.get('new'):
                text = f"NEW {track_id}"

            text_size = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)[0]
            cv2.rectangle(frame, (x1, y1 - text_size[1] - 10),
                          (x1 + text_size[0], y1), color, -1)
            cv2.putText(frame, text, (x1, y1 - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            stats_text = f"{confidence:.0%} ({hits}h)"
            cv2.putText(frame, stats_text, (x1, y2 + 20),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–∏ –Ω–∞–≤–µ–¥–µ–Ω–∏–∏
            if 'match_score' in det:
                match_text = f"M: {det['match_score']:.2f}"
                cv2.putText(frame, match_text, (x1, y2 + 40),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)

        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = f"–õ—é–¥–µ–π: {len(detections)} | –ê–∫—Ç–∏–≤–Ω—ã—Ö —Ç—Ä–µ–∫–æ–≤: {len(self.active_tracks)} | –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {sum(1 for d in detections if d.get('recovered', False))}"
        cv2.putText(frame, stats, (10, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

        # –í—Ä–µ–º—è
        time_text = datetime.now().strftime("%H:%M:%S")
        cv2.putText(frame, time_text, (frame.shape[1] - 120, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

        # –õ–µ–≥–µ–Ω–¥–∞
        legend = [
            ("üü¢ –°—Ç–∞–±–∏–ª—å–Ω—ã–π (>10 –ø–æ–ø–∞–¥–∞–Ω–∏–π)", (0, 255, 0)),
            ("üü° –ù–æ–≤—ã–π (<10 –ø–æ–ø–∞–¥–∞–Ω–∏–π)", (255, 255, 0)),
            ("‚Üª –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π", (255, 0, 255))
        ]

        y_offset = 60
        for text, color in legend:
            cv2.putText(frame, text, (10, y_offset),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            y_offset += 20

    def get_current_frame(self):
        """–ü–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä"""
        if not self.processed_queue.empty():
            return self.processed_queue.get()
        return None

    def get_statistics(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"""
        stable_tracks = sum(1 for t in self.active_tracks.values() if t['hits'] > 10)

        return {
            'current_count': self.current_count,
            'today_unique': len(self.today_unique),
            'session_unique': len(self.session_unique),
            'active_tracks': len(self.active_tracks),
            'stable_tracks': stable_tracks,
            'lost_tracks': len(self.lost_tracks),
            'avg_track_age': np.mean([t['age'] for t in self.active_tracks.values()])
            if self.active_tracks else 0
        }

    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞"""
        self.running = False
        if self.cap:
            self.cap.release()
        print("‚úì –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")